---
layout: post
title: 线性回归与逻辑回归
description: 
category: blog
---

##引入
假设有一组房屋销售的数据如下

![lr-1.png](http://mazhiyuan.github.io/images/lr-1.png "")

根据这个数据集作图得到

![lr-2.png](http://mazhiyuan.github.io/images/lr-2.png "")

如果来了一个新的面积，我们如何得到它可能的售价呢？我们可以用一条曲线去拟合这些数据，然后如果有新的输入，将曲线上这个点对应的值返回。

我们假定
房屋销售记录表：输入数据，一般称为x

房屋销售价格：输出数据，一般称为y

拟合的函数： y = h(x)

训练数据的条目数：一条训练数据是由一对输入数据和输出数据组成的输入数据的维度n (特征的个数，#features)

这个例子的特征是两维的，结果是一维的。然而回归方法能够解决特征多维，结果是一维多离散值或一维连续值的问题。 

##回归
线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以由前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。 

我们用X1，X2..Xn 去描述feature里面的分量，比如x1=房间的面积，x2=房间的朝向，等等，我们可以做出一个估计函数： 

![lr-3.png](http://mazhiyuan.github.io/images/lr-3.png "")

  θ在这儿称为参数，在这的意思是调整feature中每个分量的影响力，就是到底是房屋的面积更重要还是房屋的地段更重要。为了如果我们令X0 = 1，就可以用向量的方式来表示了： 
  
![lr-4.png](http://mazhiyuan.github.io/images/lr-4.png "")

我们程序也需要一个机制去评估我们θ是否比较好，所以说需要对我们做出的h函数进行评估，一般这个函数称为损失函数，描述h函数不好的程度，我们称这个函数为J函数

我们可以得到这样一个错误函数：

![lr-5.png](http://mazhiyuan.github.io/images/lr-5.png "")

这个错误估计函数是去对x(i)的估计值与真实值y(i)差的平方和作为错误估计函数，前面乘上的1/2是为了在求导的时候，这个系数就不见了。

如何调整θ以使得J(θ)取得最小值有很多方法，其中有最小二乘法，是一种完全是数学描述的方法，第二种是梯度下降法。
###误差函数的概率解释
假设根据特征的预测结果与实际结果有误差![lr-13.png](http://mazhiyuan.github.io/images/lr-13.png "")，那么预测结果![lr-14.png](http://mazhiyuan.github.io/images/lr-14.png "")和真实结果![lr-15.png](http://mazhiyuan.github.io/images/lr-15.png "")满足下式：

![lr-16.png](http://mazhiyuan.github.io/images/lr-16.png "")

 一般来讲，误差满足平均值为0的高斯分布，也就是正态分布。那么x和y的条件概率也就是 
 
![lr-17.png](http://mazhiyuan.github.io/images/lr-17.png "")

这样就估计了一条样本的结果概率，然而我们期待的是模型能够在全部样本上预测最准，也就是概率积最大。注意这里的概率积是概率密度函数积，连续函数的概率密度函数与离散值的概率函数不同。这个概率积成为最大似然估计。我们希望在最大似然估计得到最大值时确定θ。那么需要对最大似然估计公式求导，求导结果既是 

![lr-18.png](http://mazhiyuan.github.io/images/lr-18.png "")

这就解释了为何误差函数要使用平方和。

当然推导过程中也做了一些假定，但这个假定符合客观规律。 
##梯度下降
 梯度下降法是按下面的流程进行的：

1. 首先对θ赋值，这个值可以是随机的，也可以让θ是一个全零的向量。

2. 改变θ的值，使得J(θ)按梯度下降的方向进行减少。

看下面的图例

![lr-6.png](http://mazhiyuan.github.io/images/lr-6.png "")

这是一个表示参数θ与误差函数J(θ)的关系图，红色的部分是表示J(θ)有着比较高的取值，我们需要的是，能够让J(θ)的值尽量的低。也就是深蓝色的部分。θ0，θ1表示θ向量的两个维度。

在上面提到梯度下降法的第一步是给θ给一个初值，假设随机给的初值是在图上的十字点。

 然后我们将θ按照梯度下降的方向进行调整，就会使得J(θ)往更低的方向进行变化，如图所示，算法的结束将是在θ下降到无法继续下降为止。
 
![lr-7.png](http://mazhiyuan.github.io/images/lr-7.png "")

当然，可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，可能是下面的情况：

![lr-8.png](http://mazhiyuan.github.io/images/lr-8.png "")

上面这张图就是描述的一个局部最小点，这是我们重新选择了一个初始点得到的，看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点  

   下面的公式描述了梯度下降的过程，对于我们的函数J(θ)求偏导J：
   
![lr-9.png](http://mazhiyuan.github.io/images/lr-9.png "")

下面是更新的过程，也就是θi会向着梯度最小的方向进行减少。θi表示更新之前的值，-后面的部分表示按梯度方向减少的量，α表示步长，也就是每次按照梯度减少的方向变化多少。

![lr-10.png](http://mazhiyuan.github.io/images/lr-10.png "")

一个很重要的地方值得注意的是，梯度是有方向的，对于一个向量θ，每一维分量θi都可以求出一个梯度的方向，我们就可以找到一个整体的方向，在变化的时候，我们就朝着下降最多的方向进行变化就可以达到一个最小点，不管它是局部的还是全局的。

用更简单的数学语言进行描述步骤2是这样的：

![lr-11.png](http://mazhiyuan.github.io/images/lr-11.png "")

倒三角形表示梯度，按这种方式来表示，θi就不见了，大大简化了数学描述。

##最小二乘法
 将训练特征表示为X矩阵，结果表示成y向量，仍然是线性回归模型，误差函数不变。那么θ可以直接由下面公式得出

![lr-12.png](http://mazhiyuan.github.io/images/lr-12.png "")

但此方法要求X是列满秩的，而且求矩阵的逆比较慢。

 